{
  
    
        "post0": {
            "title": "Music Science üéµüß™",
            "content": "March 14, 2022 . Introduction . Who am I? &#129489;&#127995; . Hello üëã! . My name is Mark Andal, and I am a 4th year computer engineering student at UCLA. üë®‚Äçüî¨üîßüîåüíª . One of my interests is music üé∂üéµüéº, and so I thought it would be cool to explore it by asking: . What Makes a Song Popular? &#128293; . In this project, I hope to be doing analysis on what makes a hit song. Some questions I&#39;ll be asking are: . Are there shared similarities based on a song&#39;s metadata (i.e. bpm, variability, etc)? | What motivates people to listen to these types of songs? | Are there specific target values for these songs that songwriters and producers aim for? | . As someone who considers music to be an important part of their life and someone who is interested in music production (though I have not dabbled in it yet), learning and analyzing trends related to music sounds like a very fun project. Researching this type of information could also help others to understand certain statistics and why they hear and potentially enjoy very similar trending and top songs. There is definitely no equation for creating a top song, but knowing the data and contextualizing it could influence the music industry. . Data Sources &#128202; . I will be utilizing data from Spotify, scraped and collected from users. Specifically, I will be using these two sources, both obtained from kaggle: . Spotify All Time Top 2000s DatasetThis dataset contains audio statistics of the top 2000 tracks on Spotify. The data contains about 15 columns each describing the track and it&#39;s qualities. Songs released from 1956 to 2019 are included from some notable and famous artists like Queen, The Beatles, Guns N&#39; Roses, etc. This data is extracted from the Spotify playlist - Top 2000s on PlaylistMachinery(@plamere) using Selenium with Python. More specifically, it was scraped from http://sortyourmusic.playlistmachinery.com/. . | Top Spotify Songs from 2010-2019 DatasetThe top songs BY YEAR in the world by spotify. This dataset has several variables about the songs and is based on Billboard. Data was extracted from:http://organizeyourmusic.playlistmachinery.com/ Additionally, I may consider using the Spotify API to collect more data on each song if necessary. . | Scope &amp; Visualizations &#128269; . I hope to find general summary statistics for all the songs (mean, median, mode, max, etc) for the metadata categories. I would like to do comparisons by year for them and try to find trends or correleations with the metadata. This will probably include a lot of line/bar/scatter plots and histograms. . Predictions and Insight &#128161; . Analyzing this data as well as other potential areas of interest (such as how this differs globally) would provide insights and trends for popular songs. Breaking down the relationships and correlations would, for example, demonstrate that people prefer songs that have a good &#39;danceability&#39; rating and a high bpm. There are lots of good insights when looking at the different information regarding these popular songs. Music is a science and an art. Understanding this can help intepret the scientific approach to this art, but it also will help to gain a deeper appreciation to music making. . Methods . What is the Data I&#39;m using? . Per my data sources section, I obtained the database from kaggle by downloading the .csv file which is Spotify-2000.csv. . To parse this csv, I am mainly using pandas, which allows for easy data manipulation with dataframes as well as other computational data libraries like [numpy] (https://numpy.org/) and graphical visualization libraries like matplotlib. . Initially, I had only one main dataset - Top Spotify Songs from 2010-2019 Dataset. I focused the beginnings of my data exploration and analysis on this dataset, but as I was doing my work, I realized that using 2010-2019 bilboard doesn&#39;t encompass a variety of songs. So, I decided it would be better to include more songs from a wider range of genres. Thus, I searched more and found Spotify All Time Top 2000s Dataset. Since both datasets utilized Spotify metadata and had the same columns (just named differently), I figured it wouldn&#39;t be too difficult to apply the same data exploration work and functions to the newer dataset. Since the datasets grabs values from spotify, it would make sense to get an idea of what exactly how these columns describe the songs. So, what spotify metadata does the data include? . Spotify Metadata Column Descriptions . The kaggle website that I pulled the datasets from included descriptions, but I decided to cross-reference the official spotify website to understand what the data was describing about the songs. . Beats per Mintue (BPM) . Beats Per Minute - The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. . Energy . Energy - Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. . Danceability . Danceability - Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable. . Loudness (dB) . Loudness - The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db. . Liveness . Liveness - Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. . Length (Duration) . Duration - The duration of the track in seconds. . Acousticness . Acousticness - A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. . Speechiness . Speechiness - Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks. . Valence . Valence - A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry). . Popularity . Popularity- The higher the value the more popular the song is. . Deciding on the Exact Data . However, I then had questions on how to utilize both of the datasets like: . should I combine the two? | should I only use certain subsets of each? | would there be a skew or bias if I focused on one more than the other? | . So, I did more data exploration and had to come to a consensus. I ultimately thought I should not combine the data since there might be a heavy skew from the 600 entries for 2010-2019 as well as the genres. So, I thought it would be better to focus more on the 2000 song dataset (which encapsulates a wider variety of genres and songs) and then analyze the 2010-2019 songs separately if necessary. . Having that established, I can then focus on my data exploration and analysis. . Explorative + Analytical Process . Preliminary Data Exploration . The start of my process included basic data exploration like getting: . summary info | histograms of ranges for column categories | seeing the tops for certain columns (like genre) | graphing the trends of values | . From there, I decided on certain elements to analyze and explore deeper, focused on my main research questions (what affects the popularity of a song / what makes a hit song? does that differ based on genre? does the year / decade affect what makes a song popular ? what trends do popular songs in the past 50+ year follow? ) . General Correlation . In order to do so, I first looked at column values in correlation to popularity with numpy using the whole dataset. After figuring out that there is no strong correlation, I decided to try and see if I could break down the dataset into subsets/sub-categories to see if that would change any of the correlation with popularity. . For example, I broke down the songs into frequently popular artists, genres, the top 100 songs, and decades and compared the subsets with each other. . Artists . For artists, I found artists that appear in the dataset 10 or more times. I looked at the correlation coefficient of each of these artists to see if the makeup of their particular songs were a factor in their popularity. . Genre . For genres, I tried breaking them down into parent categories. i.e. album rock, dance rock, glam rock, etc were all categorized as rock. The same applies for other categories like pop, metal, soul, and indie. Using these 3 genres, I utilized boxplots and correlation matrices to see what genre was most popular and if genre affected correlations. . Decades . I also split the dataset into decades (1950s, &#39;60s, &#39;70s, etc...) to determine the most popular . Top 100 Songs . I decided to split the dataset into the top 100 popular songs, least 100 popular songs, and all the rest to see if the metadata had anything of significance to analyze. I utilized radar charts and correlation matrices as well to help visualize and discern important points. . Lyrics . Additionally, I wanted to look at the lyrics of the top 100 songs to see if that could provide any insight. I utilized requests and BeautifulSoup to get genius lyrics. I utilized vader (Valence Aware Dictionary for Sentiment Reasoning) to score lyrics and provide a polarity score and see if there was a trend or pattern with these top songs. . Discussion . After exploring all these different components, I utilized the visualizations I created and the data I obtained to form analysis and get a big picture. All of these in culmination helped me to form understanding and make conclusions which leads into the discussion of my work. . Results . Here, I&#39;ll be going into my code now and how I generated all of my visualizations. First, like mentioned in methods, I want to go over basic data exploration first to understand my data and get some basic insights. . Preliminary Data Exploration . Understanding the Data . First, I want to get an understanding of what the data looks like. Using head() gives us an overview of the data by looking at the first 5 rows. Now, I can see the makeup of the dataset. . Index Title Artist Top Genre Year Beats Per Minute (BPM) Energy Danceability Loudness (dB) Liveness Valence Length (Duration) Acousticness Speechiness Popularity . 0 1 | Sunrise | Norah Jones | adult standards | 2004 | 157 | 30 | 53 | -14 | 11 | 68 | 201 | 94 | 3 | 71 | . 1 2 | Black Night | Deep Purple | album rock | 2000 | 135 | 79 | 50 | -11 | 17 | 81 | 207 | 17 | 7 | 39 | . 2 3 | Clint Eastwood | Gorillaz | alternative hip hop | 2001 | 168 | 69 | 66 | -9 | 7 | 52 | 341 | 2 | 17 | 69 | . 3 4 | The Pretender | Foo Fighters | alternative metal | 2007 | 173 | 96 | 43 | -4 | 3 | 37 | 269 | 0 | 4 | 76 | . 4 5 | Waitin&#39; On A Sunny Day | Bruce Springsteen | classic rock | 2002 | 106 | 82 | 58 | -5 | 10 | 87 | 256 | 1 | 3 | 59 | . Summary Statistics . Utilizing describe() provides a table of summary statistics. This helps to get a basic idea of the range of values, the standard deviation, averages, etc. . Index Year Beats Per Minute (BPM) Energy Danceability Loudness (dB) Liveness Valence Acousticness Speechiness Popularity . count 1994.000000 | 1994.000000 | 1994.000000 | 1994.000000 | 1994.000000 | 1994.000000 | 1994.000000 | 1994.000000 | 1994.000000 | 1994.000000 | 1994.00000 | . mean 997.500000 | 1992.992979 | 120.215647 | 59.679539 | 53.238215 | -9.008526 | 19.012036 | 49.408726 | 28.858074 | 4.994985 | 59.52658 | . std 575.762538 | 16.116048 | 28.028096 | 22.154322 | 15.351507 | 3.647876 | 16.727378 | 24.858212 | 29.011986 | 4.401566 | 14.35160 | . min 1.000000 | 1956.000000 | 37.000000 | 3.000000 | 10.000000 | -27.000000 | 2.000000 | 3.000000 | 0.000000 | 2.000000 | 11.00000 | . 25% 499.250000 | 1979.000000 | 99.000000 | 42.000000 | 43.000000 | -11.000000 | 9.000000 | 29.000000 | 3.000000 | 3.000000 | 49.25000 | . 50% 997.500000 | 1993.000000 | 119.000000 | 61.000000 | 53.000000 | -8.000000 | 12.000000 | 47.000000 | 18.000000 | 4.000000 | 62.00000 | . 75% 1495.750000 | 2007.000000 | 136.000000 | 78.000000 | 64.000000 | -6.000000 | 23.000000 | 69.750000 | 50.000000 | 5.000000 | 71.00000 | . max 1994.000000 | 2019.000000 | 206.000000 | 100.000000 | 96.000000 | -2.000000 | 99.000000 | 99.000000 | 99.000000 | 55.000000 | 100.00000 | . Histograms of each numerical category . Utilizing histograms helps to get an idea visually of the range of values / scale by looking at the frequency of the values. . Some takeaways: . There is generally a positive skew for liveness, acousticness, speechiness | Negative skew for Loudness | Energy &amp; Valence seem to have the most even distribution of values | Danceability looks almost like a standard normal distribution | Interesting that the biggest clump of popular songs range in the 60/70 range | . Finding Top Genres - Bar Chart + Pie Chart . I&#39;d like to see what the top genres of the whole dataset are. Visually, I&#39;d like to generate bar graphs to see the count per genre, and a pie chart to get an idea of percentages for the top genres that occur more than 10 times. . We can see that there is a wide variety of genres. Notably, rock is very prevalent. Additionally, there are lots of genres that are international, which makes sense as this is a worldwide popular chart dataset. . Average of Values per Year - Line Plots . Here, I want to get a sense of the category values over time. So, I&#39;m grabbing the averages by year and graphing. . Now we are able to visualize how the values move. Some notable insights that can be drawn from these visualizations include: . Valence, Acousticness, and Popularity seem to have a general downward trend | Seems to have either a sudden spike or drop from 1950s to mid 1960s | . Data Analysis - General Correlation . After having done initial data exploration, I&#39;m able to go more in depth and dive into the details. To start, I want to look at general correlations with popularity. Again, I am focusing on questions like, &quot;What makes a song popular?&quot; Thus, I&#39;m trying to break down the dataset in a way that allows me to draw insights into the popularity values of the dataset. . Column values in correlation with popularity . Popularity . Popularity 1.000000 | . Loudness (dB) 0.165527 | . Danceability 0.144344 | . Speechiness 0.111689 | . Energy 0.103393 | . Valence 0.095911 | . Index 0.087442 | . Beats Per Minute (BPM) -0.003181 | . Acousticness -0.087604 | . Liveness -0.111978 | . Year -0.158962 | . There seems to be no strong correlation for any of the columns associated with popularity. In other words, according to this dataset, there is no defining characteristic of a song that determines popularity. . Data Analysis - Popular Artists . Maybe, there might be a stronger correlation when we look at the frequently occuring artists in this dataset. I&#39;ll say a popular artist has to occur 10 or more times. . Which artists most frequently occur? . Taking a look at the bar chart, there&#39;s a mix of artists from all different types of genres and time periods. . How is the correlation with popularity for individual artists? . Knowing all the popular artists, we can see how the makeups of their songs may have correlation with popularity . . It&#39;s a lot to unpack, but it&#39;s interesting to see that there is better correlation when looking at smaller subsets of the data. Particularly with the artists in this dataset, there are better indiciations of their song&#39;s metadata in correlation to popularity. Danceability, valence, and energy seem to be more indicative popularity. . Data Analysis - Genre . Cool! We found some signiciant insights when looking at artists. Now, what about genre? We found top genres in terms of frequency earlier, but let&#39;s hone in on genres when looking at the Popularity column. . What genre is most popular? . When looking at the boxpot, we can see that there&#39;s a wide range of values and the standard deviations for these genres. However, when looking at metal, soul, and hiphop, the averages are the highest, while indie is clearly the lowest. Additionally, soul has the smallest IQR and general range of values. . Now understanding these selected genres with respect to Popularity, let&#39;s look at the correlations. . Does genre affect the popularity correlation? . . With genre, it doesn&#39;t seem like there are strong correlations. Maybe we can find some relationships by looking at other aspects. . What if look at the averages of the other columns and compare? . Visualizing genre averages and comparing . To visualize the genre column averages, I thought it would be a good idea to utilize a radar chart. In this radar chart, I plotted 6 categories that I thought were most important (and for the reason that including other columns like BPM would offset the scale and distract from the overall visualization). . Looking at the radar chart, we can see that hiphop seems to be very prevalent, with the highest averages in all but acousticness. Hiphop wasn&#39;t the most popular genre, but it is a top contender with a high median and average. It is interesting to see that there seems to be high averages compared to the other genres. However, that connection and relationship isn&#39;t as strong when looking at the opposite. Specifically, indie doesn&#39;t seem to be the lowest for any of these values. . Data Analysis - Decades . Okay! We looked at genres, what if we separate the data into decades? Again, how would that affect the popularity? Can we draw any important conclusions from this? . Which decade is most popular? . Looking at the boxplots, there&#39;s not a wide difference with the median or averages between the decades. . Does decade affect the popularity correlation? . . Looking at the top column&#39;s correlations with popularity, there&#39;s not much of a strong correlation from any of the decades besides the 1950s. However, this could be attributed to the small sample size. . Data Analysis - Most / Least / Other Popular Songs . Well, what about the top 100 popular songs? Or the least 100 popular songs? And the songs in between? Let&#39;s take a look and see if we can find anything from those subcategories. . Do the top 100 songs have anything in common? . It&#39;s tough to see if the group of top songs have shared similarities, as there are wide ranges for a lot of a majority of these columns. Why don&#39;t we compare with the other songs in the dataset? . Visualizing averages and comparing . For danceability, energy, and valence, there are subtle, but incremental differences in the averages. Similar to the artist correlation, these 3 categories seem to show some relationship, as the most popular songs have the highest danceability, energy, and valence. On the other hand, the least popular songs have the lowest averages for those 3 categories, and the other songs all have average values in between. . Do these affect the popularity correlation? . pop_corr_matrix = top100_df.corr() pop_corr_matrix[[&#39;Popularity&#39;]].sort_values(by=&#39;Popularity&#39;,ascending=False) . . Popularity . Popularity 1.000000 | . Year 0.311735 | . Acousticness 0.305628 | . Speechiness 0.191763 | . Danceability 0.129425 | . Loudness (dB) 0.017308 | . Valence -0.053037 | . Index -0.078517 | . Beats Per Minute (BPM) -0.111687 | . Liveness -0.196869 | . Energy -0.243869 | . pop_corr_matrix = all_others_df.corr() pop_corr_matrix[[&#39;Popularity&#39;]].sort_values(by=&#39;Popularity&#39;,ascending=False) . . Popularity . Popularity 1.000000 | . Year 0.211013 | . Loudness (dB) 0.146701 | . Beats Per Minute (BPM) 0.084476 | . Energy 0.073797 | . Danceability 0.050467 | . Speechiness 0.017568 | . Liveness 0.017155 | . Valence -0.021804 | . Index -0.117545 | . Acousticness -0.137683 | . pop_corr_matrix = bottom100_df.corr() pop_corr_matrix[[&#39;Popularity&#39;]].sort_values(by=&#39;Popularity&#39;,ascending=False) . . Popularity . Popularity 1.000000 | . Speechiness 0.093735 | . Energy 0.084852 | . Loudness (dB) 0.048413 | . Beats Per Minute (BPM) 0.026035 | . Liveness 0.021218 | . Index 0.016853 | . Danceability -0.087916 | . Acousticness -0.098543 | . Valence -0.118626 | . Year -0.169123 | . Again, there&#39;s no strong correlation with popularity for these subsets. Even with the top 100 most popular songs, it seems there&#39;s a lot of variance. But, the radar charts give us some relationship. . Data Analysis - Lyrics . Finally, one last thing that could be interesting to look at are the song lyrics of the most popular songs! Let&#39;s analyze the lyrics and get a polarity score to see if that can tell us anything. . Do the top / bottom 100 songs have any similarities with the polarity of their lyrics? How do these two groups compare? . Looking at the polarity, within each category of top/bottom 100 songs, there&#39;s no real trend that can be distinguished. However, when comparing the two, there&#39;s a big difference. The median and mean are in the positives (with median being close to +1.00) for the top 100 songs, whereas the median and mean are in the negatives. . Discussion . General Correlation . When looking at the data as a whole, again, there is no defining characteristic of a song that determines popularity when taking a look at the popoularity correlations. . Popular Artists . When looking at popular artists, 3 categories - danceability, valence, and energy - seem to be more indicative of popularity. Since the correlation and the exact category varies on artists, there&#39;s no particular hard conclusion that can be formed. However, within the makeup of a certain artists&#39; song, they can possibly focus on that category since there seems to be a relationship with popularity. Michael Jackson, for example, has danceability most correlated with popularity, and it is an interesting find since there are iconic moves associated with some of his songs. Though it can be subjective, an artist could find what works best with their music to create more popular songs. . Genres . Genre doesn&#39;t seem to affect the popularity of songs. Although there are, according to this dataset, more or less popular genres, doing comparisons within the genre is more logical. Further exploration could be looking at the most and least popular songs within the genres and seeing how they compare to the genre&#39;s averages and such. . Decades . Like genre, decades doesn&#39;t seem to affect the popularity of songs. . Most / Least / Other Popular Songs . This separation seems to bring more insights with comparisons. There were clear distinctions between the most, least, and other popular songs when looking at the radar chart. Utilizing this method when looking at the other individual categories, like artists, genres, decades, or any other form of separation and splitting could provide for further conclusions. . Lyrics . Using vader polarity sentiment to analyze the top and bottom 100 popular songs is an example of utilizing the most/least method. In this case, we were able to discover that there is a significant difference with the median and mean of the two groups. Though it was with a relatively smaller subset, analyzing the lyrics of the top and bottom popular songs for the other subcategories like genres, artists, decades, etc could too bring about more insightful conclusions. . Final Wrapup . All in all, since there&#39;s such a wide variety of songtypes, it is difficult to generalize one trend or defining characteristic as to what makes a song popular. However, breaking down a song into its individual components (as songs commonly are) like genre, artist, etc can help to identify more focused scopes to identify how the song fits within that frame. For example, songs by Michael Jackson seem to be popular because of danceability, but there are virtually an endless amount of ways to zoom into the finer details of his songs to see what else make them popular. . Additionally, what makes a song &#39;popular&#39; is very subjective. With the popularity ratings gathered within this dataset, there is no particular standard accepted everywhere, as this is just attributed to one song-ranking source - Bilboard. And furthermore, the song&#39;s metadata are rated according to Spotify, which could vary widely across different sources. . All things considered, music is just as much a science as it is an art. There is no one true equation to make a popular song, but there are certain elements that can elevate a song&#39;s popularity when contextualized and compared to other songs. In the case of this dataset, there are higher averages for elements like danceability, energy, lyrical polarity, and in general seeing why certain songs are more popular than others. . Ultimately, Music Science sought to explore, uncover, and analyze this dataset and see what makes a song popular. But most importantly, Music Science sought to bring about appreciation to the beauty that is music and that there are a plethora of ways to create music. Though songs may, in the end, be more popular than others, it is the listener that chooses what music is most beautiful to them. . General Spotify / Music Analysis Inspiration + Research . https://web.stanford.edu/~elenatuz/courses/stats32-aut2021/Spotify_analysis.html https://www.stat.cmu.edu/capstoneresearch/spring2020/315files/team3.html https://towardsdatascience.com/visualizing-spotify-songs-with-python-an-exploratory-data-analysis-fc3fae3c2c09 https://medium.com/geekculture/spotify-data-visualization-and-analysis-using-python-4af81c5531a7 . For Radar Charts . https://betterdatascience.com/radar-charts-matplotlib-plotly/ . https://towardsdatascience.com/visualizing-spotify-songs-with-python-an-exploratory-data-analysis-fc3fae3c2c09 . For Boxplots . https://www.stat.cmu.edu/capstoneresearch/spring2020/315files/team3.html - boxplot . For Genius Lyrics requests . https://medium.com/swlh/how-to-leverage-spotify-api-genius-lyrics-for-data-science-tasks-in-python-c36cdfb55cf3 .",
            "url": "https://make-a-mark.github.io/dh140_assignment9_blog/2022/03/14/Music_Science.html",
            "relUrl": "/2022/03/14/Music_Science.html",
            "date": " ‚Ä¢ Mar 14, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Assignment 3",
            "content": "Pandas and plotting exercises . In Week 2, you used a dataset from the CORGIS website. You may have used either the Python, CSV, or JSON data files. . For this assignment, use the CSV file format for the same category of data that you used previously. . df = pd.read_csv(&#39;video_games.csv&#39;) pd.read_csv(&#39;video_games.csv&#39;) . Title Features.Handheld? Features.Max Players Features.Multiplatform? Features.Online? Metadata.Genres Metadata.Licensed? Metadata.Publishers Metadata.Sequel? Metrics.Review Score ... Length.Main + Extras.Average Length.Main + Extras.Leisure Length.Main + Extras.Median Length.Main + Extras.Polled Length.Main + Extras.Rushed Length.Main Story.Average Length.Main Story.Leisure Length.Main Story.Median Length.Main Story.Polled Length.Main Story.Rushed . 0 Super Mario 64 DS | True | 1 | True | True | Action | True | Nintendo | True | 85 | ... | 24.916667 | 29.966667 | 25.000000 | 16 | 18.333333 | 14.333333 | 18.316667 | 14.500000 | 21 | 9.700000 | . 1 Lumines: Puzzle Fusion | True | 1 | True | True | Strategy | True | Ubisoft | True | 89 | ... | 9.750000 | 9.866667 | 9.750000 | 2 | 9.616667 | 10.333333 | 11.083333 | 10.000000 | 3 | 9.583333 | . 2 WarioWare Touched! | True | 2 | True | True | Action,Racing / Driving,Sports | True | Nintendo | True | 81 | ... | 3.850000 | 5.666667 | 3.333333 | 11 | 2.783333 | 1.916667 | 2.933333 | 1.833333 | 30 | 1.433333 | . 3 Hot Shots Golf: Open Tee | True | 1 | True | True | Sports | True | Sony | True | 81 | ... | 0.000000 | 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0 | 0.000000 | . 4 Spider-Man 2 | True | 1 | True | True | Action | True | Activision | True | 61 | ... | 12.766667 | 17.316667 | 12.500000 | 12 | 10.483333 | 8.350000 | 11.083333 | 8.000000 | 23 | 5.333333 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1207 Secret Files: Tunguska | True | 1 | True | True | Adventure | True | NaN | True | 71 | ... | 11.500000 | 11.750000 | 11.500000 | 2 | 11.250000 | 8.800000 | 12.566667 | 8.716667 | 16 | 6.816667 | . 1208 Fading Shadows | True | 1 | True | True | Action,Adventure | True | NaN | True | 62 | ... | 0.000000 | 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0 | 0.000000 | . 1209 Hail to the Chimp | True | 1 | True | True | Action,Strategy | True | NaN | True | 51 | ... | 0.000000 | 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0 | 0.000000 | . 1210 Secret Files: Tunguska | True | 2 | True | True | Adventure | True | NaN | True | 64 | ... | 11.500000 | 11.750000 | 11.500000 | 2 | 11.250000 | 8.800000 | 12.566667 | 8.716667 | 16 | 6.816667 | . 1211 Chicken Hunter | True | 1 | True | True | Action | True | NaN | True | 39 | ... | 0.000000 | 0.000000 | 0.000000 | 0 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0 | 0.000000 | . 1212 rows √ó 36 columns . rows, columns = df.shape print(&#39;# of rows =&#39;, rows) print(&#39;# of columns =&#39;, columns) . # of rows = 1212 # of columns = 36 . # What are the column names of the dataframe? column_names = list(df.columns) print(column_names) . [&#39;Title&#39;, &#39;Features.Handheld?&#39;, &#39;Features.Max Players&#39;, &#39;Features.Multiplatform?&#39;, &#39;Features.Online?&#39;, &#39;Metadata.Genres&#39;, &#39;Metadata.Licensed?&#39;, &#39;Metadata.Publishers&#39;, &#39;Metadata.Sequel?&#39;, &#39;Metrics.Review Score&#39;, &#39;Metrics.Sales&#39;, &#39;Metrics.Used Price&#39;, &#39;Release.Console&#39;, &#39;Release.Rating&#39;, &#39;Release.Re-release?&#39;, &#39;Release.Year&#39;, &#39;Length.All PlayStyles.Average&#39;, &#39;Length.All PlayStyles.Leisure&#39;, &#39;Length.All PlayStyles.Median&#39;, &#39;Length.All PlayStyles.Polled&#39;, &#39;Length.All PlayStyles.Rushed&#39;, &#39;Length.Completionists.Average&#39;, &#39;Length.Completionists.Leisure&#39;, &#39;Length.Completionists.Median&#39;, &#39;Length.Completionists.Polled&#39;, &#39;Length.Completionists.Rushed&#39;, &#39;Length.Main + Extras.Average&#39;, &#39;Length.Main + Extras.Leisure&#39;, &#39;Length.Main + Extras.Median&#39;, &#39;Length.Main + Extras.Polled&#39;, &#39;Length.Main + Extras.Rushed&#39;, &#39;Length.Main Story.Average&#39;, &#39;Length.Main Story.Leisure&#39;, &#39;Length.Main Story.Median&#39;, &#39;Length.Main Story.Polled&#39;, &#39;Length.Main Story.Rushed&#39;] . . # What are the datatypes of each column? # dtypes print(df.dtypes) # for column in range(columns): # data = df.iloc[0][column] # print(column_names[column] + &#39; : &#39; + str(type(data))) # for column in df.rows[0]: # print(column + &#39; : &#39; + str(type(column))) . Title object Features.Handheld? bool Features.Max Players int64 Features.Multiplatform? bool Features.Online? bool Metadata.Genres object Metadata.Licensed? bool Metadata.Publishers object Metadata.Sequel? bool Metrics.Review Score int64 Metrics.Sales float64 Metrics.Used Price float64 Release.Console object Release.Rating object Release.Re-release? bool Release.Year int64 Length.All PlayStyles.Average float64 Length.All PlayStyles.Leisure float64 Length.All PlayStyles.Median float64 Length.All PlayStyles.Polled int64 Length.All PlayStyles.Rushed float64 Length.Completionists.Average float64 Length.Completionists.Leisure float64 Length.Completionists.Median float64 Length.Completionists.Polled int64 Length.Completionists.Rushed float64 Length.Main + Extras.Average float64 Length.Main + Extras.Leisure float64 Length.Main + Extras.Median float64 Length.Main + Extras.Polled int64 Length.Main + Extras.Rushed float64 Length.Main Story.Average float64 Length.Main Story.Leisure float64 Length.Main Story.Median float64 Length.Main Story.Polled int64 Length.Main Story.Rushed float64 dtype: object . . #collapse-output # Look at the first 2 rows of the dataframe # row1 = df.iloc[0] # row2 = df.iloc[1] # print(row1) # print(row2) df.head(2) # Look at the last 2 rows of the dataframe # second_last_row = df.iloc[rows-2] # last_row = df.iloc[rows-1] # print(second_last_row) # print(last_row) df.tail(2) . . Title Features.Handheld? Features.Max Players Features.Multiplatform? Features.Online? Metadata.Genres Metadata.Licensed? Metadata.Publishers Metadata.Sequel? Metrics.Review Score ... Length.Main + Extras.Average Length.Main + Extras.Leisure Length.Main + Extras.Median Length.Main + Extras.Polled Length.Main + Extras.Rushed Length.Main Story.Average Length.Main Story.Leisure Length.Main Story.Median Length.Main Story.Polled Length.Main Story.Rushed . 1210 Secret Files: Tunguska | True | 2 | True | True | Adventure | True | NaN | True | 64 | ... | 11.5 | 11.75 | 11.5 | 2 | 11.25 | 8.8 | 12.566667 | 8.716667 | 16 | 6.816667 | . 1211 Chicken Hunter | True | 1 | True | True | Action | True | NaN | True | 39 | ... | 0.0 | 0.00 | 0.0 | 0 | 0.00 | 0.0 | 0.000000 | 0.000000 | 0 | 0.000000 | . 2 rows √ó 36 columns . df.describe() . Features.Max Players Metrics.Review Score Metrics.Sales Metrics.Used Price Release.Year Length.All PlayStyles.Average Length.All PlayStyles.Leisure Length.All PlayStyles.Median Length.All PlayStyles.Polled Length.All PlayStyles.Rushed ... Length.Main + Extras.Average Length.Main + Extras.Leisure Length.Main + Extras.Median Length.Main + Extras.Polled Length.Main + Extras.Rushed Length.Main Story.Average Length.Main Story.Leisure Length.Main Story.Median Length.Main Story.Polled Length.Main Story.Rushed . count 1212.000000 | 1212.000000 | 1212.00000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | ... | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | 1212.000000 | . mean 1.658416 | 68.828383 | 0.50316 | 17.393894 | 2006.820132 | 13.653177 | 26.250426 | 11.225371 | 44.415842 | 9.396163 | ... | 12.731491 | 18.866048 | 12.103548 | 13.995875 | 10.319788 | 8.465952 | 11.051966 | 8.280941 | 24.882838 | 6.974697 | . std 1.204377 | 12.956266 | 1.06985 | 5.018972 | 1.050653 | 19.397156 | 51.598941 | 13.493717 | 154.837893 | 11.176371 | ... | 23.979610 | 42.917021 | 23.355374 | 57.333257 | 20.895117 | 9.691745 | 14.092617 | 9.502600 | 87.382770 | 7.964522 | . min 1.000000 | 19.000000 | 0.01000 | 4.950000 | 2004.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 25% 1.000000 | 60.000000 | 0.09000 | 14.950000 | 2006.000000 | 3.562500 | 4.000000 | 3.025000 | 1.000000 | 2.600000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | . 50% 1.000000 | 70.000000 | 0.21000 | 17.950000 | 2007.000000 | 8.858333 | 12.000000 | 8.000000 | 6.000000 | 6.708333 | ... | 7.291667 | 8.000000 | 7.000000 | 1.000000 | 6.283333 | 6.575000 | 8.000000 | 6.041667 | 3.000000 | 5.341667 | . 75% 2.000000 | 79.000000 | 0.46000 | 17.950000 | 2008.000000 | 16.033333 | 27.600000 | 13.783333 | 25.000000 | 11.366667 | ... | 16.112500 | 21.033333 | 15.000000 | 7.000000 | 12.941667 | 11.033333 | 14.508333 | 10.533333 | 14.000000 | 9.312500 | . max 8.000000 | 98.000000 | 14.66000 | 49.950000 | 2008.000000 | 279.733333 | 476.266667 | 126.000000 | 2300.000000 | 120.200000 | ... | 291.000000 | 478.933333 | 291.000000 | 1100.000000 | 291.000000 | 72.383333 | 135.583333 | 70.000000 | 1100.000000 | 70.000000 | . 8 rows √ó 25 columns . df.Title . 0 Super Mario 64 DS 1 Lumines: Puzzle Fusion 2 WarioWare Touched! 3 Hot Shots Golf: Open Tee 4 Spider-Man 2 ... 1207 Secret Files: Tunguska 1208 Fading Shadows 1209 Hail to the Chimp 1210 Secret Files: Tunguska 1211 Chicken Hunter Name: Title, Length: 1212, dtype: object . review_scores = df[&#39;Metrics.Review Score&#39;] review_scores.plot() . &lt;AxesSubplot:&gt; . # print(review_scores.loc[:10]) print(review_scores.loc[:9]) . 0 85 1 89 2 81 3 81 4 61 5 67 6 88 7 75 8 68 9 46 Name: Metrics.Review Score, dtype: int64 . # as well as the matching 10 elements of a different column that has interesting text first_10 = review_scores.loc[:9] print(first_10) titles = df[&#39;Title&#39;].loc[:9] print(titles) . 0 85 1 89 2 81 3 81 4 61 5 67 6 88 7 75 8 68 9 46 Name: Metrics.Review Score, dtype: int64 0 Super Mario 64 DS 1 Lumines: Puzzle Fusion 2 WarioWare Touched! 3 Hot Shots Golf: Open Tee 4 Spider-Man 2 5 The Urbz: Sims in the City 6 Ridge Racer 7 Metal Gear Ac!d 8 Madden NFL 2005 9 Pokmon Dash Name: Title, dtype: object . # and make a bar plot with the text values horizontally and the numeric values as the bar heights title_score = df.loc[[0,1,2,3,4,5,6,7,8,9],[&#39;Title&#39;,&#39;Metrics.Review Score&#39;]] title_score.plot(kind=&#39;bar&#39;, x=&#39;Title&#39;) . &lt;AxesSubplot:xlabel=&#39;Title&#39;&gt; . title_score.plot(kind=&#39;barh&#39;, x=&#39;Title&#39;) . &lt;AxesSubplot:ylabel=&#39;Title&#39;&gt; . # and change at least two aesthetic elements (colors, labels, titles, ...) title_score_plot = title_score.plot(kind=&#39;barh&#39;, x=&#39;Title&#39;) title_score_plot.set_title(&quot;Review Score of 10 Video Games&quot;, color=&#39;purple&#39;) title_score_plot.set_xlabel(&#39;Review Score&#39;,fontsize=14, color=&#39;red&#39;) title_score_plot.set_ylabel(&#39;Video Game Title&#39;,fontsize=14, color=&#39;yellow&#39;) . Text(0, 0.5, &#39;Video Game Title&#39;) . Free form section . Choose another type of plot that interests you from the pandas.DataFrame.plot documentation [look at the &#39;kind&#39; parameter] and make a new plot of your dataset values using the plot type | . Copy some of your analysis from the Week 2 assignment into new cells below | Clean them up if desired, and make sure that you translate them to work with your new pandas dataframe structure here if needed | Create several plots to complement and extend your analysis | . For annotating bar chart . max_play_per_year_dict = {2004: [&#39;Metal Gear Acid&#39;, 25.383333333333333], 2005: [&#39;Animal Crossing: Wild World&#39;, 168.96666666666667], 2006: [&#39;Monster Hunter Freedom&#39;, 279.73333333333335], 2007: [&#39;Monster Hunter Freedom 2&#39;, 136.01666666666668], 2008: [&#39;Animal Crossing: City Folk&#39;, 191.25]} max_play_per_year_df = pd.DataFrame(max_play_per_year_dict) pd.DataFrame(max_play_per_year_dict) # df.iloc[1].apply(int) max_play_game_list = [] year_list = [] year = 2004 for pair in max_play_per_year_dict.values(): pair_list = [pair[0], pair[1]] year_list.append(year) pair_list_year = [pair[0], pair[1], year] max_play_game_list.append(pair_list) year += 1 clean_data_frame = pd.DataFrame(max_play_game_list, columns=[&#39;game&#39;, &#39;max play&#39;]) print(clean_data_frame) ax = clean_data_frame.plot(kind=&#39;bar&#39;, x=&#39;game&#39;) for index, p in enumerate(ax.patches): ax.annotate(str(year_list[index]), (p.get_x() * 1.005, p.get_height() * 1.005)) # practice doing the color depending on the year . game max play 0 Metal Gear Acid 25.383333 1 Animal Crossing: Wild World 168.966667 2 Monster Hunter Freedom 279.733333 3 Monster Hunter Freedom 2 136.016667 4 Animal Crossing: City Folk 191.250000 . console_game_over90_dict = { &#39;Nintendo DS&#39;: [[&#39;Mario Kart DS&#39;, 91], [&#39;Advance Wars: Dual Strike&#39;, 90], [&#39;The Legend of Zelda: Phantom Hourglass&#39;, 90], [&#39;Chrono Trigger&#39;, 92]], &#39;X360&#39;: [[&#39;Gears of War&#39;, 94], [&#39;The Elder Scrolls IV: Oblivion&#39;, 94], [&quot;Tom Clancy&#39;s Ghost Recon: Advanced Warfighter&quot;, 90], [&#39;Halo 3&#39;, 94], [&#39;Call of Duty 4: Modern Warfare&#39;, 94], [&#39;Forza Motorsport 2&#39;, 90], [&#39;Guitar Hero II&#39;, 92], [&#39;Rock Band&#39;, 92], [&#39;Mass Effect&#39;, 91], [&#39;BioShock&#39;, 96], [&#39;The Orange Box&#39;, 96], [&#39;Grand Theft Auto IV&#39;, 98], [&#39;Gears of War 2&#39;, 93], [&#39;Fallout 3&#39;, 93], [&#39;Rock Band 2&#39;, 92]], &#39;Nintendo Wii&#39;: [[&#39;The Legend of Zelda: Twilight Princess&#39;, 95], [&#39;Super Mario Galaxy&#39;, 97], [&#39;Resident Evil 4&#39;, 91], [&#39;Metroid Prime 3: Corruption&#39;, 90], [&#39;Super Smash Bros.: Brawl&#39;, 93], [&#39;Rock Band&#39;, 92], [&#39;?kami&#39;, 90]], &#39;PlayStation 3&#39;: [[&#39;Call of Duty 4: Modern Warfare&#39;, 94], [&#39;The Elder Scrolls IV: Oblivion&#39;, 93], [&#39;Rock Band&#39;, 92], [&#39;Grand Theft Auto IV&#39;, 98], [&#39;Metal Gear Solid 4: Guns of the Patriots&#39;, 94], [&#39;LittleBigPlanet&#39;, 95], [&#39;Fallout 3&#39;, 90], [&#39;Rock Band 2&#39;, 91], [&#39;BioShock&#39;, 94]], &#39;Sony PSP&#39;: [[&#39;God of War: Chains of Olympus&#39;, 91]]} game_score_pair_list = [] game_score_console_list = [] console_game_dict = {} for console in console_game_over90_dict: for game_score_pair in console_game_over90_dict[console]: new_list = [game_score_pair[0], game_score_pair[1], console] game_score_console_list.append(new_list) game_score_pair_list.append(game_score_pair) if console not in console_game_dict: console_game_dict[console] = [game_score_pair[0]] else: console_game_dict[console].append(game_score_pair[0]) # print(game_score_pair_list) # print(console_game_over90_dict.values()) console_game_console_over90_df = pd.DataFrame(game_score_console_list, columns=[&#39;game&#39;, &#39;review score&#39;, &#39;console&#39;]) # console_game_console_over90_plot = console_game_console_over90_df.groupby(&#39;console&#39;).plot(kind=&#39;barh&#39;, x=&#39;game&#39;, y=&#39;review score&#39;) for title, group in console_game_console_over90_df.groupby(&#39;console&#39;): group.plot(kind=&#39;barh&#39;, x=&#39;game&#39;, y=&#39;review score&#39;, title=title) # console_game_console_over90_plot.set_title(str(console) + &#39;games over 90 review score&#39;, color=&#39;purple&#39;) # print(console_game_over90_df) # game_score_plot = console_game_over90_df.plot(kind=&#39;barh&#39;, x=&#39;game&#39;, figsize=(12,12)) . . from matplotlib.patches import Patch console_game_over90_df = pd.DataFrame(game_score_pair_list, columns=[&#39;game&#39;, &#39;review score&#39;]) # print(console_game_over90_df) barcolors = [] for i in console_game_over90_df.index: if str(console_game_over90_df.loc[i,&#39;game&#39;]) in console_game_dict[&#39;Nintendo DS&#39;]: barcolors.append(&#39;blue&#39;) elif str(console_game_over90_df.loc[i,&#39;game&#39;]) in console_game_dict[&#39;Sony PSP&#39;]: barcolors.append(&#39;red&#39;) elif str(console_game_over90_df.loc[i,&#39;game&#39;]) in console_game_dict[&#39;X360&#39;]: barcolors.append(&#39;green&#39;) console_game_dict[&#39;X360&#39;].remove(str(console_game_over90_df.loc[i,&#39;game&#39;])) elif str(console_game_over90_df.loc[i,&#39;game&#39;]) in console_game_dict[&#39;Nintendo Wii&#39;]: barcolors.append(&#39;gray&#39;) console_game_dict[&#39;Nintendo Wii&#39;].remove(str(console_game_over90_df.loc[i,&#39;game&#39;])) elif str(console_game_over90_df.loc[i,&#39;game&#39;]) in console_game_dict[&#39;PlayStation 3&#39;]: barcolors.append(&#39;black&#39;) colours = {&quot;Nintendo DS&quot;: &quot;blue&quot;, &quot;X360&quot;: &quot;green&quot;, &quot;Nintendo Wii&quot;: &quot;gray&quot;, &quot;PlayStation 3&quot;: &quot;black&quot;, &quot;Sony PSP&quot;: &quot;red&quot;} console_game_over90_plot = console_game_over90_df.plot(kind=&#39;bar&#39;, x=&#39;game&#39;, y=&#39;review score&#39;, figsize=(12,12), color=barcolors) console_game_over90_plot.legend( [ Patch(facecolor=colours[&#39;Nintendo DS&#39;]), Patch(facecolor=colours[&#39;Nintendo Wii&#39;]), Patch(facecolor=colours[&#39;PlayStation 3&#39;]), Patch(facecolor=colours[&#39;Sony PSP&#39;]), Patch(facecolor=colours[&#39;X360&#39;]) ], [&quot;Nintendo DS&quot;, &quot;Nintendo Wii&quot;, &quot;PlayStation 3&quot;, &quot;Sony PSP&quot;, &quot;X360&quot;] ) print(console_game_over90_df.loc[0,&#39;game&#39;]) print(console_game_dict[&#39;Nintendo DS&#39;]) print(barcolors) . Mario Kart DS [&#39;Mario Kart DS&#39;, &#39;Advance Wars: Dual Strike&#39;, &#39;The Legend of Zelda: Phantom Hourglass&#39;, &#39;Chrono Trigger&#39;] [&#39;blue&#39;, &#39;blue&#39;, &#39;blue&#39;, &#39;blue&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;green&#39;, &#39;gray&#39;, &#39;gray&#39;, &#39;gray&#39;, &#39;gray&#39;, &#39;gray&#39;, &#39;gray&#39;, &#39;gray&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;black&#39;, &#39;red&#39;] . for title, group in console_game_console_over90_df.groupby(&#39;console&#39;): group.plot.hist(title=title) . . console_game_console_over90_df.plot.hist() . &lt;AxesSubplot:ylabel=&#39;Frequency&#39;&gt; .",
            "url": "https://make-a-mark.github.io/dh140_assignment9_blog/2022/03/06/assignment03.html",
            "relUrl": "/2022/03/06/assignment03.html",
            "date": " ‚Ä¢ Mar 6, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://make-a-mark.github.io/dh140_assignment9_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://make-a-mark.github.io/dh140_assignment9_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://make-a-mark.github.io/dh140_assignment9_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://make-a-mark.github.io/dh140_assignment9_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}